---
layout: single
title: "cs231n Lecture11 Detection and segmentation"
categories: "computervision"
tag: "cs231n"
typora-root-url: ../
sidebar:
  nav: "docs"
---

###### abstract

지금까지는 Image classification Task를 수행하는 모델에 대해 중점적으로 다뤄 보았다. 이제부터는 다양한 computer vision task를 다뤄볼 것이다. 대표적으로 Semantic segmentation (의미론적 분할), Localization, object detection(객체 탐지), Instance segmentation같은 task를 수행하는 모델들에 대해서 배워볼 것이다.

#### Semantic Segmentation

지금까지 Image classification task를 위주로 다뤘다면 이번 Lecture에서는 다양한 computer vision task를 다뤄볼 예정이다. Segmentation, Localization, Detection등 다양한 Computer Vision Tasks와 이 문제들을 어떻게 CNN을 활용해 접근해볼지에 대해서 다뤄볼 것이다. Semantic segmentation 문제에서는 출력으로 image의 모든 pixel에 category를 부여한다.

![img](/images/2024-02-04-cs231n11/clip_image002.jpg)

Image classification처럼 image 1장에 category하나를 부여하는 것이 아니라 1장의 모든 pixel 단위로 category를 부여하는 것이다. 그러나 Semantic segmentation은 같은 종류의 category끼리 구분하지 않아 위 image에 있는 2마리의 cow object를 구분하지 못한다. Cow라고 labeling된 pixel 덩어리만 얻을 수 있다. 이는 Semantic segmentation의 단점이다. 나중에 배울 Instance Segmentation 이 이를 해결할 수 있다.

![img](/images/2024-02-04-cs231n11/clip_image004.jpg)

Semantic Segmentation 문제에 접근해볼 수 있는 방법 중 하나는 Classification을 통한 접근 이다. Input image를 아주 작은 단위로 쪼개 각각 classification문제로서 연산한다고만 생각해볼 수 있다. 이 방법은 한눈에 봐도 좋은 방법은 아니다. Cost가 엄청나게 크기 때문이다. 모든 pixel에 대해서 작은 영역으로 쪼개고 이 모든 영역을 forward/backward pass하는 일은 상당히 비효율적이다. 게다가 여러 pixel이 같은 category의 feature를 공유한다면 classification도 제대로 수행하지 못할 수 있다. 이보다 개선된 방법이 있다. 바로 Fully convolutional network, FCN이다.

###### Fully convolutional Network(FCN)

![image-20240204192608457](/images/2024-02-04-cs231n11/image-20240204192608457.png)

FCN은 Image를 pixel 단위로 나눠 독립적으로 분류하지 않고 FC-Layer가 없는 Convolution layer만으로 구성된 Network를 활용한다. zero padding을 수행하는 Conv layer들을 쌓아 올리면 image의 공간정보를 손실하지 않는다. 이 Network의 출력 Tensor는 C(depth, category의 수) x H x W 이다. 이 출력 Tensor는 input image의 모든 pixel에 대해 classification scores를 개별적으로 부여한 값이다. 이 Network를 학습시키려면 우선 모든 pixel의 classification loss를 계산하고 더한 다음 평균값을 취해야 한다. 그리고 기존처럼 backpropagation을 수행하면 될 것이다.

Fully convolutional Network는 Train data를 만들기 위해 input image의 모든 pixel에 labeling 을 해야 한다. 모든 pixel의 category를 알고 있다고 가정하는 것이다. Loss function을 디자인하기 위해서 출력의 모든 pixel에 Cross entropy를 적용한다.

Cross entropy loss란 softmax function을 통해 모델의 출력을 확률 분포로 변환한 후 이 확률 분포와 Ground Truth 확률 분포 간의 차이를 계산하는 것이 바로 cross entropy loss이다.

출력의 모든 pixel과 Ground Truth 간의 Cross entropy를 연산하고 이 값들을 모두 더하거나 평균을 내서 Loss를 계산한다. 또는 mini-batch 단위로 연산할 수도 있다. Segmentation에서는 labeling을 위해 class의 수가 고정되어 있다.

그러나 이 Network의 경우 input image의 Spatial size를 계속 유지시켜야 하므로 비용이 매우 크다. Convolution의 channel이 64/128/256인 경우 고해상도의 image가 input으로 들어오면 감당해야 할 연산과 memory가 너무 커서 감당할 수 없다. 실제로 이런 FCN은 흔하지 않다.

 ![img](/images/2024-02-04-cs231n11/clip_image002-1707042375504-28.jpg)

대신 feature map을 Down sampling/Upsampling 한 FCN이 대부분이다. Spatial resolution (공간 해상도) 전체를 가지고 convolution을 수행하기 보다는 Original resolution에서 Conv layer는 소량만 사용하고 Max pooling, Stride등을 활용해 feature map을 Down sampling하는 것이다. Classification Network와 구조가 유사해 보이나 Image Classification Network는 FC-layer가 있는 반면 FCN은 Spatial resolution을 다시 키워서 결국 다시 input image의 Original resolution과 같아지게 한다. 이 방법으로 계산 효율을 높일 수 있다. 또한 Network가 low resolution을 처리하도록 하여 Network를 더 깊게 만들 수 있다.

###### Unpooling

![image-20240204192657035](/images/2024-02-04-cs231n11/image-20240204192657035.png)

Upsampling이 Network안에서 어떻게 동작하는지 궁금할 수도 있다. Network의 feature map의 size를 키울 수 있는 전략이 무엇일까? 대표적으로는 unpooling하는 방법이 있다. “nearest neighbor unpooling”과 “Bed of Nails unpooling”이 그것이다. 2X2 grid(격자)를 4X4 grid로 만들기 위해 해당하는 receptive field로 값을 복사하거나 0값을 추가한다. Bed of Nails라고 불리는 이유는 zero region은 평평하고 non-zero region은 바늘처럼 값이 튀기 때문이다.

![img](/images/2024-02-04-cs231n11/clip_image002-1707042423704-30.jpg)

Max unpooling이라는 방법도 있다. 대부분의 Network는 대칭적인 경향이 있다. Down sampling / upsampling의 비율이 대칭적인 것이다. Unpooling과 pooling을 연관 짓는 방법이다. Down sampling시에는 Max pooling에 사용했던 요소들을 잘 기억하고 있어야 한다. Upsampling시에 bed of nails upsampling과 유사하되 같은 자리에 모든 값을 특정 grid에 삽입하는 것이 아니라 이전 Maxpooling에서 선택된 위치에 맞게 넣어주는 것이다. Semantic segmentation에서는 모든 pixel의 class를 잘 classification할 수 있어야 한다. 예측한 segmentation 결과에서 object간의 경계가 명확할수록 좋다. 하지만 Max pooling을 수행하면 feature map에 Imbalance가 발생한다. 2X2 maxpooling을 수행한 후에 output값들이 input의 어느 부분에서 왔는지 모르게 되어 공간정보를 잃어버리는 불균형이 발생한다는 뜻이다. Maxpooling후의 feature map만 봐서는 이 값들이 Receptive field 중 어디에서 왔는지 알 수 없다. Unpooling 시에 기존 Max pooling에서 뽑아온 자리로 값을 넣어주면 spatial information을 좀 더 detail하게 다룰 수 있다. 잃어버린 공간정보를 다시 찾아주는 것이다.

###### Transpose Convolution

![image-20240204192754386](/images/2024-02-04-cs231n11/image-20240204192754386.png)

또 다른 방법으로는 Transpose Convolution이 있다. 이전의 3가지 Unpooling방법들은 고정된 함수이고 별도로 학습을 시키지는 않는다. 하지만 stride convolution의 경우는 어떨까? Stride convolution은 어떤 식으로 Downsampling을 수행해야 할지 Network가 학습할 수 있다. Conv layer의 filter의 크기와 stride같은 hyperparameter를 조절하면 backpropagation과정을 거치면 서 parameter가 최적화되기 때문이다. 이와 유사하게 Upsamling에서도 학습 가능한 방법이 있다. 바로 Transpose Convolution이다.

 ![image-20240204192844524](/images/2024-02-04-cs231n11/image-20240204192844524.png)

Stride convolution과 반대로 input이 2X2이고 output이 4X4이다. Transpose convolution에서는 filter와 input의 내적을 연산하는 것이 아니라 입력 값이 filter에 곱해지는 weight의 역할을 한다. 출력에서는 Transpose convolution간에 Receptive field가 겹칠 수 있다. 이렇게 겹치는 경우 겹치는 두 값을 더해서 출력한다. 이 과정을 반복해서 끝마치면 학습 가능한 Upsampling을 수행한 것이다. Transpose Convolution을 계산하는 과정이 마치 convolution 연산을 거꾸로 계산하는 것과 같아보여서 Deconvolution이라고도 불리는 것 같다. 그러나 수학적으로 Deconvolution은 정확히 convolution의 역 연산을 일컫는다. Convolution 연산의 수식은 다음과 같다.
$$
filter * input = output
$$
따라서 deconvolution은 filter와 output을 알고 있는 상태에서 input을 구하는 것이다. 여기서 Transpose convolution과 Deconvolution의 개념적인 차이가 발생한다. Deconvolution은 convolution 연산에 사용한 filter와 output을 알고 있어야 하며, 역 연산을 통해서 input을 재현하려는 목적이라고 볼 수 있다. 그러나 Transpose Convolution에서 사용하는 filter는 Conv layer와 공유하는 것이 아니다. Transpose Convolution layer가 학습을 통해서 filter를 찾아간다는 점에서 차이가 있다. 그럼 왜 이런 식의 upsampling과정이 transpose convolution이라고 불리는 것일까? 그걸 이해하기 위해서 우선 convolution 연산을 컴퓨터가 어떤 방식으로 수행하는지 알아볼 필요가 있다.

###### Convolution matrix

![img](/images/2024-02-04-cs231n11/clip_image001.jpg)

4x4 input data를 stride 1, 3x3 filter를 활용해 2x2 output data로 만드는 과정을 생각해보자. 사람은 filter를 input data에 sliding시켜 2x2 output data를 연산할 수 있지만 컴퓨터는 다르다.

![img](/images/2024-02-04-cs231n11/clip_image005.jpg)![img](/images/2024-02-04-cs231n11/clip_image004-1707042552685-35.jpg)

컴퓨터는 3x3 filter를 4x16형태의 matrix로, 4x4 input data를 16x1 column vector로 변환한다. 해야 한다. 4x4 input data, 3x3 filter만으로는 행렬 곱 연산을 수행할 수 없기 때문이다.

![image-20240204193010762](/images/2024-02-04-cs231n11/image-20240204193010762.png)

그리고 4x16 input matrix와 16x1 filter matrix에 대해 행렬 곱 연산을 수행해 4x1 output vector를 생성한다. 이 vector를 2x2 matrix로 표현한 것이다. 이것이 컴퓨터가 Conv 연산을 수행하는 방법이다. 이제 Transpose Convolution matrix에 대해 알아보자.

###### Transpose Convolution Matrix

![image-20240204193034289](/images/2024-02-04-cs231n11/image-20240204193034289.png)

Convolution 연산의 역 과정을 수행하기 위해서 아래와 같은 연산을 수행한다.
$$
 (Transpose\,filter\,matrix) * (input\,vector) = output vector
$$
Convolution연산을 통해 Downsampling시킨 feature map의 Spatial resolution을 복구하기 위해 filter matrix의 크기를 Transpose시켜서 작은 크기의 입력에도 큰 크기의 출력이 가능하도록 한다. 이 모든 경우는 stride가 1인 Transpose Convolution 연산을 수행하는 경우를 생각한 것이다. Stride가 2가 되는 경우 위의 모든 방식은 바뀌어야 한다. Stride 2 transpose convolution은 convolution과 근본적으로 다른 연산이 된다. 하지만 이런 경우도 포함해서 transpose convolution 연산이라 이름 붙였다. Transpose convolution은 결국 filter matrix의 크기를 MXN에서 NXM으로 변환하는데 중점을 두기 때문이다. 이런 원리를 이용해 input data의 spatial resolution을 키우기 때문에 Transpose convolution이라는 이름이 붙은 것이다.

![image-20240204193117384](/images/2024-02-04-cs231n11/image-20240204193117384.png)

이런 Semantic segmentation 구조는 상당히 일반적이다. Network의 내부를 살펴보면 image를 분할하기 위해 Network 내부에 Downsampling/Upsampling을 수행하는 거대한 convolution network가 있다. Downsampling은 stride convolution이나 pooling을 사용하며 upsampling은 transpose convolution 혹은 다양한 종류의 unpooling연산 기법을 사용한다. 그리고 모든 pixel에 대한 cross entropy를 계산하면 Network 전체를 end-to-end로 학습시킬 수 있다. End- to-end란 data의 input부터 output까지의 전체 과정을 하나의 통합된 system으로 구성하여 학습하는 것을 의미한다.

#### Classification + Localization

Image가 어떤 category에 속하는지 뿐만 아니라 실제 object가 어디에 위치하는지를 알고 싶을 수 있다. Classification + Localization은 object detection과는 구별된다. localization이란 computer vision 분야에서 이미지 내에서 특정 객체의 위치를 찾는 Task를 의미한다. 따라서 Localization 문제에서는 내가 관심있는 object가 단 하나뿐이라고 가정한다. 기본적으로 image내에서 object 하나만 찾아서 Label을 매기고 위치를 찾아낸다. Architecture의 기본 구조는 다음과 같다.

![img](/images/2024-02-04-cs231n11/clip_image002-1707042707316-39.jpg)

FC-layer의 마지막 4096-dim vector layer가 2개의 layer와 연결되는데 하나는 Class score로 연결되어 category를 결정한다. 그리고 나머지 하나는 width, height, x, y 4개의 element를 가진 vector와 연결된다. 4개의 element는 bounding box의 위치를 나타낸다. 이런 식으로 Network는 Class score, bounding box의 좌표를 나타내는 2가지 출력값을 반환한다. 이 Network를 학습시킬 때는 Loss가 2가지 존재한다. 그리고 이 task는 fully supervised setting을 가정하므로 Train dataset에는 category label과 해당 object의 bounding box GT(ground truth)를 동시에 갖고 있어야 한다. 자, 이제 2가지의 Loss function이 있다. 우선 class score를 예측하기 위한 softmax loss가 있을 것이고 bounding box GT와 예측한 box의 좌표 간의 차이를 측정하는 Loss도 있을 것이다. L2 Loss로 BBox Loss를 가장 쉽게 디자인 가능하다. 이 Loss들은 모두 예측한 BBox의 좌표와 BBOX GT간의 차이에 대한 regression loss이다.

만약 Model이 잘못 분류한 object에 대해서 BBox가 생성되면 어찌되는지에 대한 의문이 있을 수 있다. 이렇게 되면 예측한 BBox와 GT간의 오차를 학습하는데 문제가 생길 수 있기 때문이다. 하지만 실제로 큰 문제가 생기지는 않는다. 그래서 많은 사람들이 두 Loss를 동시에 학습시킨다. 이 문제는 BBox를 하나만 예측하지 않고 BBox를 category마다 하나씩 예측해 GT에 해당하는 object를 예측한 BBox만 Loss와 연결시키는 방법으로 해결할 수 있다.

이 2개의 losses를 합친 Loss를 Multi-task Loss라고 한다. 우선 Gradient를 구하기 위해 Network weight들의 각각의 미분값을 계산해야 한다. 이제는 Loss가 2개이니 미분값도 2개이고 이 2개를 모두 최소화시켜야 한다. 실제로는 두 Losses의 가중치를 조절하는 hyperparameter가 있다. 두 Losses에 각각 hyperparameter를 곱한 뒤 더한 값이 최종 Loss다. 그리고 두 Losses의 가중 합에 대한 Gradient를 계산하는 것이다. 하지만 두 Losses의 비중을 결정하는 것은 상당히 까다로운 문제라 이 가중치는 hyperparameter로써 우리가 설정해줘야 한다. hyperparameter를 설정할 때 Loss가 아니라 실제 Model의 성능 지표를 보면 도움이 된다.

![image-20240204193210069](/images/2024-02-04-cs231n11/image-20240204193210069.png)

앞 쪽의 큰 Conv Network는 고정시킨 다음 2갈래의 FC-layer만 학습시키는 방법도 괜찮을 것이다. Transfer Learning의 관점에서 보면 fine tuning을 하면 항상 성능이 더 좋아진다. ImageNet으로 학습시킨 Model을 가지고 우리가 가진 dataset에 적용한다면 적절한 Fine tuning이 도움이 될 것이다. 실제로 사람들은 Network를 Freeze하고 두 FC-layer를 학습시킨 뒤 FC-layer가 수렴하면 다시 합쳐서 전체 system을 Fine tuning하는 방법을 많이 사용한다.

 ![img](/images/2024-02-04-cs231n11/clip_image002-1707042740417-44.jpg)

BBox와 같이 image 내의 어떤 위치를 예측한다는 idea는 classification + localization문제 이외에도 다양한 문제에 적용해볼 수 있다. 그 중 하나는 human pose estimation이다. Human pose estimation문제에서는 사람 image가 input으로 들어간다. 출력은 이 사람의 각 관절의 위치이다. 이 Network는 사람의 pose를 예측한다. 이런 문제를 풀기 위해 일부 data set은 14개의 관절의 위치로 사람의 pose를 정의한다.

이 Network의 input은 사람의 image이며 output은 각 관절에 해당하는 14개의 좌표 값이다. 예측된 14개의 점에 대해서 regression loss를 연산하고 backpropagation로 학습시킨다. 이를 위해 다양한 regression loss를 적용할 수 있다.

Regression loss는 cross entropy나 softmax가 아닌 Losses를 의미한다. 가령 L2, L1, smooth L1 Loss등이 있다. Classification과 Regression의 일반적인 차이점은 결과가 categorical인지 continuous인지이다. 가령 고정된 개수의 category가 있고 model은 이를 결정하기 위한 class score를 출력하는 경우라면 지난 강의에서 배웠던 Cross entropy, softmax, SVM margin loss를 사용할 수 있을 것이다. 반면 출력이 연속적인 값인 경우라면 다른 종류의 Loss를 사용해야 한다. L2, L1과 같은 Losses를 사용할 수 있을 것이다.

#### Object Detection(객체 감지)

Object detection은 논할 주제가 매우 많은 computer vision에서 가장 중요한 문제 중 하나다. Object detection에 관련된 주제로 하루 종일 세미나를 진행할 수 있을 정도로 역사가 깊고 테크닉도 다양하다. 우리는 object detection + deep learning에 관련한 주요 idea만 간략하게 짚고 넘어갈 것이다.

![img](/images/2024-02-04-cs231n11/clip_image002-1707042791916-46.jpg)

위의 image는 semantic segmentation, Classification + Localization, object detection를 비교한 것이다. Object detection 문제에서도 고정된 category가 존재한다. Object detection의 task는 input image가 주어지면 image에 나타나는 object의 BBox와 해당하는 category를 예측하는 것이다. 예측해야 하는 BBox의 수가 input image에 따라 달라지기 때문에 Classification + Localization과는 차이가 있다. 각 image에 몇 개의 object가 있을지가 미지수이기 때문에 꽤나 어려운 문제이다.

![image-20240204193334824](/images/2024-02-04-cs231n11/image-20240204193334824.png)

Pascal VOC Dataset에서 object detection task를 수행하는 성능의 진보 과정을 보여주는 그래프이다. 2012년까지 object detection의 성능은 점점 정체되♘다가 Deep learning이 도입된 이후로는 성능이 매우 빠르게 증가했다. State of the art(현 최고 수준의 기술)은 이제 object detection을 매우 쉽게 수행한다.

![image-20240204193345945](/images/2024-02-04-cs231n11/image-20240204193345945.png)

Object detection은 Localization과는 다르게 object의 수가 image마다 다르다. 가령 좌측 상단의 고양이 image는object가 하나라 4개의 숫자만 예측하면 되지만 마지막 오리 사진의 경우 한 마리당 4개씩 아주 많은 값을 예측해야 할 것이다.

Object detection 문제를 해결하기 위해 사람들이 많이 시도했던 방법 1은 sliding window이다. 가령 image의 왼쪽 밑에서 작은 영역을 추출해 그 작은 입력만 CNN의 input으로 넣는다. CNN은 이 작은 영역에 대해서 Classification을 수행할 것이다. 그리고 어떤 결론을 도출할 것이다. 단, 여기에서는 배경 category를 하나 추가해야 한다. Network가 배경이라고 예측한다면 이 곳은 어떤 category에도 속하지 않는다는 것을 의미한다. 그렇다면 이 방법의 문제는 무엇일까?

바로 영역을 어떻게 추출할지가 문제가 될 수 있다. Image에 objects가 몇 개가 존재하는지도, 어디에 존재하는지도 알 수가 없다. 크기가 어떤지도 알 수 없을 것이다. 따라서 이런 brute force(무식한) 방식의 sliding window를 하려면 너무나 많은 경우의 수가 존재한다. 작은 영역 하나 하나마다 거대한 CNN을 통과시키려면 이 때의 연산량은 예상할 수 조차 없다.

![image-20240204193357754](/images/2024-02-04-cs231n11/image-20240204193357754.png)

대신에 Region Proposals라는 방법이 있다. 사실 이 방식은 Deep learning을 사용하지 않고 전통적인 신호처리 기법을 사용한다. Region Proposal Network(RPN)은 object가 있을 법한, 가령 2000개의 BBox를 생성한다. 이미지 내에서 object가 있을 법한 후보 Region Proposals를 찾아내는 다양한 방법이 있겠지만 RPN은 image 내에 뭉텅진(blobby) 곳들을 찾아낸다. 이 지역들은 object가 있을지도 모르는 후보 영역들이다. 이런 알고리즘은 비교적 빠르게 동작한다. Region Proposal을 만들어낼 수 있는 방법에는 Selective Search가 있다. Selective Search은 N개(ex.2000)의 Region Proposal을 만들어낸다. CPU로 2초간 Selective Search를 돌리면 객체가 있을만한 N개의 Region Proposal을 만들어낸다. 이 방법은 noise가 아주 심하다. 대부분은 실제 object가 아니겠지만 Recall(모든 실제 object를 정확하게 감지하는 비율)은 높다. 따라서 image에 object가 존재한다면 selective search의 Region proposal 안에 속할 가능성이 높다.

이제는 무식하게 image내의 모든 위치와 scale을 전부 고려하는 것이 아니라 우선 Region proposal Networks를 적용하고 객체가 있을 법한 Region proposal을 얻어낸다. 그리고 이 Region proposals를 CNN의 input으로 하는 것이다. 이 방법을 사용하면 연산량을 다루기 훨씬 수월하다. 모든 위치와 scale을 전부 고려하는 방법보다는 낫다고 볼 수 있다. 지금까지 말한 idea가 R-CNN(2014)이라는 논문에 등장한다.

###### R-CNN

![image-20240204193425627](/images/2024-02-04-cs231n11/image-20240204193425627.png)

Image가 주어지면 Region proposal을 얻기 위해 Region proposal Network를 수행한다. Region proposal은 Region of Interest(ROI)라고도 한다. Selective search를 통해 2000개의 사각형 ROI를 얻어낸다. ROI가 대부분 사각형인 이유는 non-region things(사각형이 아닌 것들)의 크기를 warp(조정)하는 것은 까다롭기 때문이다. 여기서는 각 ROI의 size가 각양각색이라는 점이 문제가 될 수 있다. 추출된 ROI로 CNN Classification을 수행하려면 FC-Layer 등으로 인해서 보통 같은 입력사이즈로 맞춰줘야만 한다. 따라서 Region proposals을 추출하면 CNN의 입력으로 사용하기 위해서 동일한 고정된 크기로 변형시켜야 한다. 그래서 각각의 ROI를 추출하고 나서 고정된 size로 크기를 변경한다. 그리고 각각의 ROI를 CNN에 통과시킨다. 그리고 R-CNN의 경우 ROI들의 최종 classification에 SVM(supervised learning의 일종)을 사용했다. R-CNN은 ROI를 보정하기 위한 regression 과정도 거친다. ROI가 대게는 object를 잘 찾아내지만 그렇지 못한 경우도 있기 때문이다. R-CNN은 BBox의 category도 예측하지만, BBox를 보정해줄 수 있는 offset 값 4개도 예측한다. 이를 Multi-task loss로 두고 한번에 학습한다. 여기서 offsets는 ROI의 외부로 향할 수도 있어야 한다. ROI가 사람의 머리를 빼먹은 경우, 사람의 머리도 ROI에 포함시켜야 하기 때문이다.

Instant Segmentation(object detection + semantic segmentation)의 경우 ROI가 사각형이 아닌 경우도 있다. 또한 수많은 ROI 중에서 실제 object를 포함하지 않은 ROI는 배경 object로 분류된다. 이 Task는 Fully supervised이므로 train data에는 image 내의 모든 object에 대한 BBox가 있어야 한다.

R-CNN framework는 많은 문제점을 가지고 있다. 2000개의 ROI가 독립적으로 CNN input으로 들어가므로 계산 비용이 여전히 매우 높다. 그리고 image당 2000개의 ROI에 대해서 forward/backward pass를 수행하므로 학습과정 자체가 굉장히 오래 걸린다. R-CNN의 구현을 살펴보면 CNN에서 나온 Feature를 ROI의 수만큼 disk에 넣으므로 용량이 어마어마하다.

###### Fast R-CNN

![image-20240204193448529](/images/2024-02-04-cs231n11/image-20240204193448529.png)

위의 다양한 문제는 Fast R-CNN이 상당부분 해결했다. R-CNN은 각 ROI마다 CNN을 수행했지만 Fast R-CNN은 전체 image에 대해서 CNN을 수행한다. 그 결과 전체 image에 대한 고해상도 feature map을 얻을 수 있다. 여전히 selective search같은 방법으로 ROI를 연산하지만 전체 Image에서 ROI를 뜯어내는 것이 아닌 CNN feature map에 ROI를 projection 시켜 전체 image가 아닌 feature map에서 뜯어온다. 이제는 CNN의 features를 여러 ROI가 공유할 수 있다. 그 다음 FC-layer가 있으며 FC-layer는 고정된 크기의 input을 받는다. 따라서 feature map에서 뜯어온 ROI는 FC-layer의 input에 맞게 size를 조정해야 한다. 이 단계가 바로 ROI pooling layer이다.

![image-20240204193502164](/images/2024-02-04-cs231n11/image-20240204193502164.png)

Feature map에서 뜯어온 ROI의 size를 warp(변형)하고 나면 FC-layer의 input으로 넣어서 classification score와 Linear regression offset을 연산할 수 있다. Fast R-CNN을 학습할 때는 두 Loss를 합쳐 Multi-task Loss로 backpropagation을 수행한다. 이 방법을 통해 전체 Network를 합쳐서 동시에 학습시킬 수 있다. ROI pooling은 max pooling과 유사하나 자세히 다루지는 않을 것이다.

###### R-CNN vs fast R-CNN

![image-20240204193515718](/images/2024-02-04-cs231n11/image-20240204193515718.png)

Train time에는 Fast R-CNN이 R-CNN보다 10배 가량 더 빠르다. Fast R-CNN은 각 ROI마다 CNN 연산을 수행해 각각 feature map을 출력하는게 아니라 모든 ROI가 이미 CNN연산을 거친 feature map을 서로 공유하기 때문이다. Test time의 fast R-CNN은 정말 빠르다. Region proposal을 연산하는 시간이 대부분을 차지할 정도다. 2000개의 ROI를 selective search로 연산하는데 2초 가량 걸린다. ROI를 연산한 이후 CNN을 거치는 과정은 모든 ROI가 공유하기 때문에 1초도 걸리지 않는다. 따라서 fast R-CNN은 Region Proposal을 계산하는 구간이 병목이다.

###### Faster R-CNN

고맙게도 Faster R-CNN은 Network가 ROI를 직접 만들 수 있다.

![image-20240204193601615](/images/2024-02-04-cs231n11/image-20240204193601615.png)

Input image 전체가 Network로 들어가서 feature map을 만든다. Faster R-CNN은 별도의 Region Proposal Network, RPN이 존재한다. RPN은 Network가 feature map을 가지고 ROI를 연산하도록 한다. RPN을 거쳐 ROI를 예측하고 나면 나머지 동작은 Fast R-CNN과 동일하다. Conv feaure map에서 ROI를 뜯어내고 이들을 나머지 Network에 통과시킨다. 그리고 mult-task loss를 이용해서 여러 Loss를 한번에 연산한다. Faster R-CNN은 4개의 loss를 한번에 학습시킨다. 이 Loss들의 균형을 맞추는 것은 상당히 까다롭다. RPN에는 2가지의 Losses가 존재한다. 첫 번째 Loss는 object가 있는지 없는지를 예측하는 것이고 두 번째 Loss는 예측한 BBox에 관한 것이다. 그리고 Faster R-CNN의 최종단에서도 2개의 Loss가 존재한다. 하나는 ROI의 classification을 결정하는 것이고 나머지 하나는 BBox regression이다. 앞서 만든 ROI를 보정해주는 역할이다.

Region proposal network의 GT가 없는데 어떻게 RPN을 학습시키는지 궁금할 수 있다. 이 부분에 대해서는 자세히 설명하지 않겠다.

Region proposal network의 classification loss는 무엇일까? RPN은 Binary classification을 수행한다. ROI 내부에 objection이 존재하는지 아닌지를 결정하는 것이다. 따라서 Loss는 binary classification Loss이다.

![image-20240204193616266](/images/2024-02-04-cs231n11/image-20240204193616266.png)

Faster R-CNN은 매우 빠르다. 빨라진 가장 큰 이유는 Network 밖에서 계산했던 ROI의 병목을 제거했기 때문이다. 이로 인해 다른 Network보다 훨씬 더 빨라지게 되♘다. Faster R-CNN에서 흥미로운 점은 ROI 자체도 학습되♘기 때문에 RPN과 우리의 새로운 data사이의 간극이 존재할 수도 있을 것이다. 이 경우 새로운 data에 맞게 ROI를 새롭게 학습시키면 된다. 지금까지 살펴본 R-CNN 계열의 Network들은 후보 ROI마다 독립적으로 연산을 수행했다(FC-layer). 이런 R-CNN 계열의 Network들을 region-based method라고 한다.

Object detection에는 R-CNN 계열 외에 다른 방법도 존재한다. 대표적인 것들이 YOLO, you only look once와 SSD, single shot detection이다. 이 둘은 거의 같은 시기에 나왔으며 각 Task를 따로 계산하지 말고 하나의 regression문제로 풀어보자는 아이디어를 가지고 있다. 거대한 CNN을 통과하면 모든 것을 담은 예측 값이 한번에 나온다. Input image가 있으면 image를 큼지막하게 나눈다. 가령 7X7 grid로 나눌 수 있을 것이다. 각 Grid cell 내부에는 Base BBox가 존재한다.

![image-20240204193625162](/images/2024-02-04-cs231n11/image-20240204193625162.png)

이 경우에는 Base BBox가 3가지 있다. 직사각형들과 정사각형이다. 실제로는 3개 이상 사용한다. 이제는 이 각 grid cell에 대해서 BBox가 있고 이를 기반으로 예측을 수행할 것이다. 우선 하나는 BBox의 offset을 예측할 수 있다. 실제 위치가 되려면 BBox를 얼마나 이동시켜야 하는지를 뜻한다. 그리고 각 BBox에 대해서 classification scores를 연산해야 한다. 이 BBox 안에 이 category에 속한 object가 존재할 가능성을 의미한다.

![image-20240204193633045](/images/2024-02-04-cs231n11/image-20240204193633045.png)

Network에 input image가 들어오면 7X7 Grid마다 (5B+C)개의 tensor를 가진다. 여기서 5는 base BBox의 offset 4개와 confidence score 1개로 구성된다. Confidence score란 신뢰도를 나타내는 0에서 1 사이의 점수이다. 여기서는 BBox내부에 category에 속한 object가 존재할 가능성을 의미한다. B는 각 grid마다 존재하는 Base BBox의 개수를 의미한다. 그리고 C는 C개의 category에 대한 classification score다.

정리하면, Network의 input은 image이고 출력은 3차원 tensor다. 그리고 이를 거대한 CNN으로 한번에 학습시킨다. 이런 방법들(YOLO/SDD)를 “single shot methods”라고 한다. 후보 base BBox와 GT objects를 매칭시키는 방법이다. 가만 보면 Faster R-CNN에서 쓰는 RPN과 유사해 보인다. 결국은 둘 다 Regression + classification 문제를 푸는 것이다. R-CNN 계열, single shot 계열이라고 나눠도 분명 겹치는 idea가 많다. Faster R-CNN은 RPN으로 먼저 Regression 문제를 풀고 ROI단위로 Classification을 하는 방식이다. 반면 single shot methods는 단 한번의 forward pass만으로 끝내 버린다. Faster R-CNN 계열의 Region based methods는 정확도가 높긴 하나 single shot methods보다는 속도가 느리다. Single shot methods는 ROI당 별도의 연산을 요구하지 않기 때문이다.

Object detection에는 아주 다양한 요소들이 존재한다. 가령 VGG, ResNet같은 다양한 base networks를 적용해볼 수도 있다. 그리고 다양한 Architecture를 선택할 수 있다. 가령 Faster R- CNN과 같은 region-based method를 적용해볼 수도 있고 Single Shot Detection 기반의 방법들도 있♘다. 그리고 아주 다양한 Hyperparameter가 존재한다. 이미지의 크기를 어떻게 할지, ROI를 몇 개로 할 지 아주 다양하다.

Cs231n강좌의 조교는 Object Detection +Image captioning을 통해 Dense captioning을 주제로 논문을 발표했다. 이 문제에서는 각 region에 대해서 category를 예측하는 것이 아니라 각 region의 Caption을 예측해야한다. 이 문제를 위해서는 각 region에 caption이 있는 dataset이 있어야 한다. 이 model을 end-to-end로 학습시켜서 이 모든 것을 동시에 예측할 수 있도록 한 것이다. 이처럼 다양한 문제들을 서로 묶어서 문제를 풀 수 있다.

#### Instance segmentation

Instance는 어떤 class, category의 개별적인 사례를 가리키는 용어이다. 따라서 Instance segmentation은 여러 개의 객체를 개별적으로 식별하고 구분하는 Task를 의미한다.

![image-20240204193725135](/images/2024-02-04-cs231n11/image-20240204193725135.png)

 

Instance segmentation은 input image가 주어지면 object별로 위치를 알아내야 한다. Object detection문제와 유사하다. 하지만 object별로 BBox를 예측하는 것이 아니라 object 별로 segmentation Mask를 예측해야 한다. image에서 각 object에 해당하는 pixel을 예측하는 문제이다. Instance Segmentation은 Semantic Segmentation과 Object Detection을 짬뽕한 것이다. Object Detection 문제처럼 여러 object를 찾고 각각을 구분해 줘야 한다. 가령 Image 내에 2마리의 개가 있으면 Instance segmentation은 이 두 마리를 구분하며 각 pixel이 어떤 object에 속하는지 전부 다 결정해 줘야 한다. 다양한 방법이 있지만 Mask R-CNN(2017)이라는 논문을 소개하겠다.

![image-20240204193735072](/images/2024-02-04-cs231n11/image-20240204193735072.png)

Mask R-CNN은 처음 input image가 CNN과 RPN을 거쳐서 ROI를 뜯어내는 것이 faster R- CNN과 유사하지만 Faster R-CNN처럼 Classification/BBox Regression을 하는 것이 아니라 각 BBox마다 Segmentation mask를 예측하도록 한다. RPN으로 뽑은 ROI 영역 내에서 각각 semantic segmentation을 수행한다. Feature map으로부터 ROI pooling(align)을 수행하면 두 갈래로 나뉜다. 상단에 보이는 1번째 갈래는 Faster R-CNN과 유사하게 각 ROI가 어떤 category에 속하는지 연산한다. 그리고 ROI의 좌표를 보정해주는 BBox Regression도 예측한다. 그리고 하단의 2번째 갈래는 Semantic Segmentation을 위한 mini-Network같이 생겼다. 각 pixel마다 object인지 아닌지를 classification한다.

![image-20240204193751768](/images/2024-02-04-cs231n11/image-20240204193751768.png)

Mask R-CNN은 오늘 배운 모든 방법들을 전부 통합시켜 우수한 성능을 자랑한다.

![image-20240204193804469](/images/2024-02-04-cs231n11/image-20240204193804469.png)

Mask-RCNN으로는 사람들의 관절의 좌표를 예측하는 부분을 추가해서 Pose estimation도 가능하다. Mask-RCNN 하나로 Object Detection, Pose estimation, Instance segmentation이 가능하다. Pose estimation을 위해서는 각 ROI 갈래 하나를 추가해서 현재 ROI 내부 object의 관절에 해당하는 좌표를 예측하도록 하면 된다. 이를 위해서는 Loss와 Layer를 하나 더 추가하면 된다. Multi-task Loss에 Loss가 하나 더 추가되는 것이다.

Mask R-CNN은 forward(전방 전달) 한 번으로 image 내에 사람이 몇 명 있는지, 어디에 있는지, 어떤 pixel에 해당하는지를 알아낼 수 있고, 뿐만 아니라 사람들의 Skeleton Estimation도 가능하다. 심지어 사람들이 아주 많이 겹쳐있는 교실 image에서도 정말 잘 작동한다. 그리고 Faster R-CNN framework기반이기 때문에 실시간에 가깝게 처리할 수 있다. 모든 것이 Forward pass 1 번으로 수행되므로 GPU에서 1초에 5장 정도 처리할 수 있다.