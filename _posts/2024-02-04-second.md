---
layout: single
title: "cs231n Lecture3 Loss Functions and Optimizations "
categories: "computervision"
tag: "cs231n"
typora-root-url: ../
sidebar:
  nav: "docs"
---



###### abstract

Loss함수의 역할과 대표적인 Loss function들을 배운 뒤, 최적의 Loss를 찾는 과정에서 발생할 수 있는 overfitting에 대비하기 위해 Regularization을 배운다. Loss를 내리는 과정인 optimization에 대해 배우며 기초적인 gradient descent의 개념과 경사하강법을 활용한 SGD에 대해서 배운다. 마지막으로 이미지에서 특징을 추출하는 방법의 과거-현재에 대해 자세하게 알려준다.

### Loss function

어떤 파라미터 행렬W가 좋은 W인지 판단하기 위해서는 W가 좋은지 나쁜지 정량화할 방법이 필요하다.

![image-20240204103652332](/images/2024-02-04-second/image-20240204103652332.png)

Loss function L_i를 정의해 각 label마다 W가 트레이닝 샘플을 얼마나 구리게 예측하였는지 정량화하고 모든 Losses를 더한 뒤 Label의 수로 나눠서 평균적인 Loss를 구한다.
$$
Loss = \frac{각 Label별 Losses의 합}{Label의 수}
$$

###### Loss function 1. Multi-class SVM loss function 

SVM: support Vector machine, 주어진 데이터를 분류하는데 사용되는 ML 알고리즘

![image-20240204104306480](/images/2024-02-04-second/image-20240204104306480.png)

만약 input image에 대해서 정답 클래스의 예측 점수가 오답 클래스의 예측 점수보다 1이상 크다면 Loss는 0으로 인정, 그렇지 않다면

(오답 클래스 – 정답 클래스+1)

은 Loss로 설정된다. 여기서 1은 safty margin이라고 하며 값이 커질수록 Loss를 0으로 설정하는 기준이 엄격해진다고 할 수 있다. Multi-class SVM loss function은 점수 자체보다는 점수 간 상대적인 차이에 중점을 두기 때문에 safty margin의 크기는 그다지 상관하지 않는다.  이렇게 설정된 각 class별 Losses를 다 더한 후, label의 수로 나누면 최종적인 Loss가 결정된다. 이 Loss를 구하는 최종 함수를 cost function이라고 한다.

![image-20240204104345245](/images/2024-02-04-second/image-20240204104345245.png)

Multi-class SVM Loss function은 그래프가 경첩과 같이 생겨 Hinge Loss라고 부르기도 한다.

![image-20240204104356173](/images/2024-02-04-second/image-20240204104356173.png)

(오답 class 예측 점수 - 정답 class 예측 점수)가 작아질수록 Hinge loss의 함수값이 커진다. Multi-class SVM loss function으로 연산한 Loss의 최대값은 이론상 무한대이며 최소값은 0이다. 일반적으로 가중치 행렬 W는 매우 작은 값으로 초기화되는데 그때 모든 클래스 예측 점수는 0에 가깝게 된다. 만약 Loss를 구하는 식이 다음과 같이 된다면

![image-20240204104410711](/images/2024-02-04-second/image-20240204104410711.png)

Loss function의 값이 확 튄다. 오답 클래스의 Losses가 나쁜 것은 더 나쁘게 되고, 좋은 것은 더 좋게 만드는 등 Loss function의 값이 많이 달라지게 된다. 따라서 어떤 Loss function을 설계하는지가 중요하다. Linear classifier가 이미지를 잘 분류한다면 Loss=0이 된다. 그렇다고 Loss가 0가 되는 가중치 행렬 W가 유일하진 않다. 행렬 W의 Losses가 0이라면 가중치 행렬 2W의 Losses 역시 0이 된다.

![image-20240204104427624](/images/2024-02-04-second/image-20240204104427624.png)

2W역시 Loss는 0이 된다. 그러나 위의 식에서 L=0이 되는 W를 구하는 것은 좋지 않을 수 있다. Training data에만 맞는(fitting) Loss function만 찾는 것이기 때문이다. Test data를 입력했을 때 image classifier가 이해하지 못할 행동을 할 수 있다.

### Regularization

![image-20240204104510803](/images/2024-02-04-second/image-20240204104510803.png)

Training data(파란색 점)으로 만들어진 파란색 선(model prediction)은 새로운 test data(녹색 점)를 잘 설명하지 못한다. Loss가 0이면 training data에 너무 딱 맞아서(overfitting) 새로운 데이터에는 적용되지 않는다. 마치 갓난아기에게 원숭이 사진 1장을 학습시킨 뒤 10여종의 새로운 원숭이를 원숭이인지 구분해보라고 시키는 것과 다를 게 없는 것이다. Regularization은 예측 모델(W)이 너무 복잡한 고차 다항식을 선택할 때 페널티를 줌으로써, 단순한 저차 다항식을 선택하도록 한다. 쉽게 설명하자면 Regularization은 예측 모델(W)이 training data에 완전히 fit하지 못하게 모델 복잡도에 penalty를 부여하는 것이다.

![image-20240204104521022](/images/2024-02-04-second/image-20240204104521022.png)

overfitting(과잉 적합)은 모델의 용량이 데이터의 용량보다 월등히 많은 경우 발생한다. 10만개의 sample로 100만개의 weight를 갱신하려 한다면 마치 똑같은 그림을 수백, 수천 번 복기하는 것과 같다. Loss function이 단순히 작아지는 방향으로 학습하다 보면 행렬 W의 특정 Wij가 너무 큰 값을 가지게 되어 과하게 구불구불한 형태의 함수가 만들어지므로 W가 너무 큰 값을 가지지 않도록 조절하는 방법 등이 있을 수 있다.

![image-20240204104532798](/images/2024-02-04-second/image-20240204104532798.png)

![image-20240204104537712](/images/2024-02-04-second/image-20240204104537712.png)

![image-20240204104542075](/images/2024-02-04-second/image-20240204104542075.png) L1 Regularization



C0: 원래 Loss값, n: 훈련 데이터 개수, λ: hyperparameter, |w|: 가중치 행렬 요소의 절댓값 Linear classifier의 함수 f(x,W)=Wx에 대해서
$$
X^T  = [1,1,1,1]\\
W_1= [1,0,0,0] \\ W_2= [0.25,0.25,0.25,0.25]
$$
에 대해서 
$$
Wx
$$
 내적 연산을 수행하면 둘 다 결과는 1이 나온다. W1의 경우 XT의 첫번째 원소 외에 다른 원소는 무시하지만 W2는 원소를 골고루 반영한다. L1의 경우 W1을 더 선호한다. L1은 W에서 0의 개수에 따라 모델 복잡도(행렬 W의 0이 아닌 수의 개수)가 달라지며 W의 원소를 대부분 0이 되게 한다. 이를 sparse solution이라고 한다. 반면 L2는 W2를 선호하는데 L2의 Regularization은 X의 모든 원소가 골고루 영향을 미치도록 한다. 이를 coarse solution이라고 한다. L2는 W의 feature를 최대한 고려하며, 동일한 점수라면 더 넓게 펼쳐져 있는 것을 선호한다. 숫자가 넓게 퍼질 때, 모델 복잡도는 덜 복잡해진다.

###### Softmax loss function (Multinomial logistic regression)

Softmax loss function은 이항의 Logistic Regression을 다차원으로 일반화시킨 함수이다. Multiclass SVM Loss는 예측 점수 자체는 고려하지 않으며, 정답 클래스의 예측 점수가 오답 클래스의 예측 점수보다 크기만을 바란다. 그러나 Softmax는 class별로 확률분포를 사용하여 예측 점수 자체에 추가적인 의미를 부여한다.

![image-20240204104817401](/images/2024-02-04-second/image-20240204104817401.png)

한 input image, 예를 들어서 고양이 image에 대해 Model이 10개의 class 예측 점수를 출력할 수 있다. 각 예측 점수는 양수, 음수가 모두 가능하다. 따라서 확률 값으로 취하기 위해 모든 score를 양수로 바꿔야 한다. 이를 위해 이 각각의 예측 점수를 모두 지수화한 뒤
$$
\frac{지수화한 정답 클래스의 예측 점수}{지수화한 모든 𝐜𝐥𝐚𝐬𝐬 예측 점수의 합} = \frac{e^sk}{\sum_{j}e^s_j}
$$
우리는 이제 sofxmax함수를 통해 확률 값을 도출하는 식까지 알아봤다. 이제는 확률 값을 최대로 추정해서 정확도가 높은 분류기를 만들어야 하는데, 여기서 Cross entropy를 사용한다. 연산한 값을 –log(X)에 삽입해 Normalization을 수행한다. Normalization이란 값들을 0~1사이의 범위로 재조정하는 scaling 기법이다. loss의 정의(W가 얼마나 나쁜 것인지 정량화)로 인해 –를 붙여준다. 따라서 정답 클래스가 정답일 확률이 낮을수록 Losses값은 점점 더 커진다.

![image-20240204105123859](/images/2024-02-04-second/image-20240204105123859.png)
$$
Losses = -log(예측 클래스가 정답일 확률)
$$
![image-20240204105230194](/images/2024-02-04-second/image-20240204105230194.png)

### Optimization

행렬W중에서 가장 성능이 좋은 W를 찾아나가는 과정 Gradient descent (경사 하강법)

--------------------------------------------------------

**1.** W를 임의의 값으로 초기화 (값을 할당함)

**2.** Loss, gradient를 연산 후, 가중치를 gradient 반대 방향으로 업데이트

**3.** gradient => 함수에서 증가하는 방향 => -gradient: 함수에서 내려가는 방향

**4.**  작업을 반복해서 국지적인 최소값에 수렴 (최종 목표는 global minimum)

-------------------------------------

-f'(x) 방향으로 가면 최저점을 찾을 수 있다 => 경사하강법의 핵심 원리: 어떤 방식으로 가중치를 갱신할까?

------------------------

**1.** Analysis descent

완전한 수식을 풀어 최저점을 찾는 방법 f'(x) 활용

입력 데이터의 차원이 낮고 Loss function이 단순한 경우에 적용

**2.** Numerical descent

조금씩 d θ 만큼 움직여 최저점을 찾는 방식

---------------------------------------------------------------------------------

Gradient Descent를 통해 최적화를 진행하는데 W를 임의의 값으로 초기화하고 True일 때 기울기를 연산하며 –(기울기) 방향으로 조금씩 이동해서 최종적인 결과를 얻는다.

여기서 step size(=learning rate)라는 hyperparameter에 따라 최적화에 도달하는 속도가 달라진다. 매 step마다 W는 조금씩 -gradient 방향으로 이동하는 식으로 W를 업데이트하는데 이를 Update rule이라고 한다.

###### SGD(stochastic gradient descent)

![image-20240204105422771](/images/2024-02-04-second/image-20240204105422771.png)

데이터 수 N이 클수록 Gradient Descent를 계산하는 시간이 오래 걸리는데 이를 보완하기 위해 데이터의 일부분인 mini-batch에 대한 gradient를 구하고 이를 epoch만큼 반복해서 구해 평균한 loss, gradient를 추정하여 W를 업데이트하는 방식을 고안했다. 이를 SGD라 한다. Mini-batch 1을 모두 학습하면 epoch 1을 완료했다고 표현한다.

### Image features

이미지를 분류하기 위해서는 이미지의 특징을 추출해야 한다. 정제되지 않은 이미지 픽셀을 입력값으로 받아 이미지의 특징 벡터를 추출해 입력 데이터로 활용할 수 있어야 한다.

수작업 특징 추출에는 3가지 유형이 있다.

----------

**1.** Color histogram

![img](/images/2024-02-04-second/clip_image002.jpg)



특정 색에 해당하는 픽셀의 수를 세어 히스토그램으로 표현

**2.** Histogram of Oriented gradient (HoG)

![image-20240204105534045](/images/2024-02-04-second/image-20240204105534045.png)

이미지를 8X8 픽셀로 나누어 그 픽셀 지역 내 가장 지배적인 edge를 찾아 edge orientation 히스토그램으로 표현한다.

**3.** Bag of Words

  ![img](/images/2024-02-04-second/clip_image006.jpg)

**1.**  이미지를 잘라 시각 단어로 표현한 후 coded book에 붙이고 K-means 알고리즘으로 1천개의 중심점을 갖도록 학습을 한다(색에 따라서 군집화).

**2.**  이미지의 시각 단어의 발생 빈도를 인코딩하여 이미지의 특징을 추출한다.

---------

![image-20240204105602063](/images/2024-02-04-second/image-20240204105602063.png)



AI 연구 초창기

이미지의 특징을 추출한 뒤 추출한 특징 벡터를 linear classification의 입력으로 사용한다. 특징이 한번 추출되면 classifier가 트레이닝되는 동안 변하지 않음

현재

특징 벡터를 따로 생성하기 보다는 데이터로부터 직접 특징을 학습하려 하며 Layer를 통해 feature를 추출함.