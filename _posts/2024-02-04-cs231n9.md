---
layout: single
title: "cs231n Lecture9 CNN architectures"
categories: "computervision"
tag: "cs231n"
typora-root-url: ../
sidebar:
  nav: "docs"
---

###### abstract

CNN의 다양한 Architectures, 주로 ImageNet Large Scale Visual Recognition Challenge(ILSVRC) 에서 해마다 우수한 성적을 거둔 image classification model들을 배운다. 후에 model들을 성능 과 메모리 같은 여러 성능 측면에서 comparsion해본 뒤 Image classification model들과 직, 간 접적으로 연결된 Architectures에 대해 배워본다.

#### CNN Architectures

###### LeNet (1998)

![image-20240204185105042](/images/2024-02-04-cs231n9/image-20240204185105042.png)

LeNet은 성공적으로 적용된 최초의 Convolution Network이다. 이미지를 입력으로 받아 stride=1 인 5X5 filter를 거쳐 몇 개의 Conv Layer와 Pooling Layer를 거친다. 그리고 맨 끝에 FC-Layer가 활용된다. 숫자 인식에서 엄청난 성공을 거두었다.

###### AlexNet(2012)

![image-20240204185152329](/images/2024-02-04-cs231n9/image-20240204185152329.png)

AlexNet은 최초의 대규모 CNN Model이다. ImageNet classification Task를 성공적으로 수행했으며 기존의 Non-DL model들을 능가하는 놀라운 성능을 보여줘 ConvNet 연구의 부흥을 일으킨 장본 인이다. 기본적으로 conv-pool-normalization 구조가 2번 반복된다. 그리고 뒤에 Conv layer가 조금 더 붙고 그 뒤에 Pooling layer, 마지막에 FC-layer가 몇 개 붙는다. 기존의 LeNet과 비교해 상당히 유사하며 Layer만 추가된 느낌이다. AlexNet은 input data의 크기가 227X227X3이다. 첫 layer를 살펴보면 11X11 filter가 stride=4로 96개 존재한다. 그렇다면 첫번째 layer의 출력 size 는 어떻게 될까?
$$
\frac{227(input data)-11(filter)}{4(stride)}+ 1 = 55
$$
Filter가 96개이므로 feature map의 출력 size는 55X55X96이다. 각 filter는 11X11X3=363개의 weight와 1개의 bias를 변수로 갖기 때문에 filter 1개의 parameter는 총 364개, filter가 96개 이 므로 첫번째 layer의 parameter 수는 약 35K이다. 두 번째 Layer는 Pooling Layer인데 stride가 2인 3X3 filter가 있다. 여기서 말하는 filter란 parameter가 없는 pooling 연산 수행을 위한 filter이다. 이 Layer의 feature map의 크기는 27X27X96이다. 그리고 끝에는 각각 4096, 4096, 1000개의 node를 가진 FC Layer를 통과해 1000개의 ImageNet class의 score로 변환된다. AlexNet의 기본 activation function은 ReLU함수이다. Local response normalization layer는 channel(색)의 normalization을 위한 것인데 요즘은 잘 사용하지 않는다. 큰 효과가 없는 것으로 알려졌기 때문이다.

![image-20240204185311809](/images/2024-02-04-cs231n9/image-20240204185311809.png)

AlexNet은 data augmentation을 자주 적용했다. 대표적으로 flipping, jittering, color norm등을 적용하였다. 또한 Dropout을 적용했다. 학습 시 Batch size는 128이다. 그리고 backpropagation 알고리즘으로 SGD Momentum을 활용했으며 초기 Learning rate는 e-2 이다. Weight decay(가중 치 감쇠)를 사용했고 마지막에는 모델 앙상블로 test 성능을 개선했다. AlexNet이 개발될 당시에 는 GPU의 memory한계로 인해 전체 layer를 GPU에 모두 담을 수 없♘다. Layer의 node가 많아 질수록 parameter가 기하급수적으로 증가하기 때문이다. 따라서 네트워크를 GPU에 분산시켜서 넣♘다. 따라서 Conv 1,2,4,5는 Depth가 48인 feature map을 base로 사용하는 셈이다. 그러나 Conv 3와 FC 6, 7, 8은 이전 계층의 전체 Feature map과 연결되어 있어 이전 layer의 전체 Depth 를 전부 가져올 수 있다. 입력 데이터 전체의 정보를 활용할 수 있게 되는 것이다.

###### ZFNet (2013)

ZFNet은 대부분 AlexNet의 hyperparameter(HP)를 개선한 모델이다. AlexNet과 같은 layer 수를 가지며 구조도 비슷하다. 다만 stride size, filter 수 같은 HP를 조절해서 AlexNet의 성능을 좀 더 개선시켰다.

#### VGGNet(2014)

![image-20240204185352924](/images/2024-02-04-cs231n9/image-20240204185352924.png)

VGGNet은 2014 ILSVRC(ImageNet Large Scale Visual Recognition Challenge)에서 준우승을 차 지한 알고리즘이다. VGGNet에는 VGG16, VGG19등 다양한 version이 존재하는데 AlexNet은 8개 의 layer를 가지지만 VGGNet은 16~19개의 layer를 가진다. 그리고 3X3 크기의 filter만 활용했 다. 이렇게 작은 filter를 유지해 주고 주기적으로 pooling을 수행하면서 전체 Network를 구성하 게 된다. 왜 작은 filter를 활용했을까? 우선 filter의 크기가 작으면 parameter의 수가 더 적어진 다. 따라서 큰 filter에 비해 layer를 많이 쌓을 수 있다. Depth가 더 깊어지는 것이다.
$$
3 * 3 * D(filter depth) * F(filter 수) * 3(layer 수) < 7*7*D*F
$$
각 model의 weight의 개수를 비교해보면 작은 필터를 사용하는 것이 weight의 수를 크게 낮추 는 것을 확인할 수 있다. 그리고 3X3 Filter를 3개 layer로 중첩해서 쌓으면 결국 7X7filter를 사 용하는 것과 실질적으로 동일한 receptive field를 가진다. Receptive field, 즉 수용영역이란 feature map의 한 유닛이 상호작용 할 수 있는 이전 layer의 input data의 영역이다.

![image-20240204185447494](/images/2024-02-04-cs231n9/image-20240204185447494.png)

3X3 filter를 사용하면 7X7 filter와 실질적으로 동일한 receptive field를 가지면서도 layer가 더 깊 어지게 할 수 있다. 이는 특징을 더 세분화해서 추출 가능하다는 의미가 되기도 한다.

네트워크의 전체 메모리 사용량을 알아보자. Parameter의 총 개수는 138M(1억 3800만)으로 AlexNet의 60M에 비해 2배 이상 많은 parameter를 사용한다. 이때, parameter와 bias는 GPU에 저장되어 있으므로 컴퓨터에서 활용해야 하는 메모리는 이미지 1장당
$$
VGGNet에 활용되는 Node의 수(24M) * 1node당 메모리(4byte)=96MB
$$
를 Forward pass과정에서 활용해야 한다. 따라서 CPU memory가 5GB라면 50장밖에 처리할 수 없는 셈이다. 심지어 Backward pass과정에서는 더 많은 메모리를 할당해야 할 것이다. Forward pass연산과정에서 layer 1의 연산을 수행한 후 연산이 끝난 이전 layer의 node memory는 회수 해도 되냐고 물어볼 수 있다. 하지만 backward pass의 chain rule이 적용되는 과정에서 대부분은 이용되기 때문에 필수적으로 저장하고 있어야 한다.

![image-20240204185533114](/images/2024-02-04-cs231n9/image-20240204185533114.png)

VGGNet의 학습과정은 전반적으로 AlexNet과 유사하다. 앙상 블 기법을 사용해 Test성능을 어느정도 끌어올렸다. 다만 Local response normalization은 사용하지 않는다. 크게 도움 이 되지 않기 때문이다. 그리고 VGG의 마지막 FC-layer인 FC7은 아주 좋은 feature representation(특성 표현)을 가지고 있는 것으로 알려져 있다. 다른 데이터에서도 feature 추출이 잘되어 일반화능력이 매우 뛰어나다.

VGGNet은 2014년 Localization분야에서는 우승을 차지했는데 Localization은 이미지의 주요 객체의 종류뿐만 아니라 객 체가 이미지의 어느 부분에 위치하는지 네모 박스를 그리는 task를 의미한다.

#### GoogleNet(2014)

2014년 ILSVRC에서 우승한 모델이다. 22개의 layer를 가지고 있으며 연산을 이전에 비해 매우 효 율적으로 진행할 수 있도록 Network가 설계되어 있다.

![image-20240204185619165](/images/2024-02-04-cs231n9/image-20240204185619165.png)

위의 사진의 파란색 블록의 층수를 세어보면 22개의 layer로 구성되어 있음을 알 수 있다. GoogleNet은 Inception module이라는 새로운 개념을 적용했다. Inception module을 여러 개 쌓 아서 GoogleNet이라는 거대한 Network를 생성한 것이다. GoogleNet은 weight를 줄여 연산을 효율적으로 진행하기 위해 FC-layer가 거의 존재하지 않는다. 따라서 전체 parameter수가 5M정도로 획기적으로 개선되었다. 이는 초창기 CNN모델인 AlexNet에 비해 가중치가 1/12 정도 수준으로 획기적으로 감소한 것이다. 그만큼 FC layer가 비효율적인 연산구조라는 의미이기도 하다. 가중치는 매우 줄어들었으나 layer의 Depth는 매우 깊어져 ILVRC 14에서 6.7%의 top-5 error로 우승한다.

![image-20240204185731650](/images/2024-02-04-cs231n9/image-20240204185731650.png)

Inception Module이란 무엇일까? Inception module을 처음 개발한 사람들은 좋은 local network topology를 만들고 싶어했다고 한다. Local Network란 지역적인 범위에서 작동하는 Network를 의미하는데 이걸 잘 작동하도록 만들고 싶♘다는 뜻이다. 그래서 Network within a Network라는 개념으로 local topology를 구현했고 이를 쌓아 올렸다. 기존에는 같은 크기의 filter를 여러 개 생성해 layer를 서로 연결시켰다면 이제는 서로 다른 크기의 다양한 filter를 병렬적으로 같은 input layer에 적용해 나온 값을 Depth방향으로 concatenate(쌓았다). 이전 layer의 입력을 받아 서 다양한 Conv 연산을 한 layer에서 수행하는 것이다. 이렇게 쌓인 여러 feature map은 하나의 tensor로 출력이 결정되고 이 하나의 출력을 다음 layer로 전달하는 것이다. 위의 사진은 매우 단 순한 방식(naive)이다. 이 방법의 문제는 무엇일까?

일단 첫번째로 계산 비용에 문제가 있다.

![image-20240204185746685](/images/2024-02-04-cs231n9/image-20240204185746685.png)

입력의 깊이가 256인 이유는 이 층이 model의 어느 부분을 가져온 것이기 때문이다. 예제를 자 세히 들여다보면 우선 128개의 1X1 filter가 있다. 192개의 3X3 filter와 96개의 5X5 filter도 존재 한다. 그리고 stride를 적절히 조절하여 입/출력 간의 spatial dimension을 유지시켜준다. Input data의 공간적인 구조를 유지시켜준다는 의미이다. 이 경우 1X1 Conv의 출력은 28X28X128이 된다. 3X3 Conv의 경우 출력이 28X28X192이 될 것이고 5X5filter는 28X28X96이 될 것이다. Pooling Layer에서는 input에서 Depth가 변하지 않는다. 모든 값들을 concatenate하면 size는 28X28X(128+92+96+256)=28X28X672가 된다. Input이 28X28X256에서 출력이 28X28X672가 된 것이다. Spatial dimension은 변하지 않았지만 depth가 엄청나게 깊어졌다.

이제 이 layer의 연산량을 살펴보자. 1X1 Conv layer는 각 픽셀마다 256번의 내적 연산을 수행하 므로 28X28X128X256번의 연산이 수행된다. 이런 식으로 3X3, 5X5 Conv의 연산량도 계산해볼 수 있다. 하나의 Inception Module에서의 전체 연산량은 854M이 된다. 매우 많은 연산량이 필요 한 것이다. 또한 layer를 거칠 때마다 Depth가 점점 늘어 점점 연산량이 늘어나게 된다. 이러한 문제를 어떻게 해결할 수 있을까?

![image-20240204185812214](/images/2024-02-04-cs231n9/image-20240204185812214.png)

GoogleNet에서 사용한 key insight는 bottleneck layer를 활용하는 것이다. Conv연산을 수행하기 에 앞서 입력을 더 낮은 차원으로 보내는 것이다. 1X1 Conv 연산을 보면 spatial dimension은 변하지 않으면서 Depth만 축소시킬 수 있다. 입력의 Depth를 더 낮은 차원으로 projection(정사영) 하는 것이다. 이를 이용해 각 Conv, Pooling 연산을 수행하기 전/후에 1X1 Conv 연산을 추가한 다. 1X1 Conv가 bottlececk layer의 역할로 추가되는 것이다. 다시 한번 연산량을 계산해보자.

![image-20240204185825939](/images/2024-02-04-cs231n9/image-20240204185825939.png)

입력은 이전과 동일하게 28X28X256이다. 1X1 Conv가 depth의 차원을 줄여주어 3X3 Conv 앞쪽 의 1X1 Conv의 출력은 28X28X64이다. 5X5 Conv와 Pooling layer에서도 동일한 작용을 한다. 계 산해보면 전체 연산량은 358M번의 연산이 수행된다. 기존의 854M보다 훨씬 줄어든 셈이다. 1X1 Conv를 활용해 전체의 연산을 획기적으로 줄이는데 성공했다. 1X1 Conv는 일부 정보에 손실이 발생할 수 있으나(애초에 모든 high dimension -> low dimension 연산은 정보의 손실이 필연적 으로 발생한다.) redundancy(불필요한 중복)이 있는 input features를 선형결합 한다고 볼 수도 있다. 엄밀한 해석이라고는 볼 수 없으나 이런 관점도 있다는 뜻이다. 오히려 1X1 Conv가 전체적 인layer를 더 깊어지게 해 더 특징을 세밀하게 추출할 수도 있다는 장점도 있다.

GoogleNet의 앞쪽은 일반적인 Network 구조이다. 초기 6개의 layer는 지금까지 봤던 일반적인 layer들이다. 초기에는 Conv, pooling연산을 몇 번 반복한다. 이 후에는 Inception module에 쌓 이는데 모두 조금씩 다른 구조를 지닌다. 그리고 마지막에는 classifier 결과를 출력한다. 연산량이 많은 FC-layer는 대부분 걷어냈고 parameter가 줄어들어도 모델이 제대로 동작함을 확인했다.

![image-20240204185835182](/images/2024-02-04-cs231n9/image-20240204185835182.png)

여기 파란색 박스 부분을 보면 추가적인 줄기가 뻗어 있는데 이들은 auxiliary classifier(보조 분 류기)이다. Average pooling, 1X1 Conv가 있으며 FC-layer도 몇 개 붙는다. 그리고 softmax Loss function을 통해 1000개의 ImageNet Class를 분류한다. Network의 끝에서만 아니라 이 두 곳에 서도 Loss를 연산하는 이유는 Network가 깊기 때문이다. Auxiliary classifier를 중간 layer마다 달 아주면 추가적인 gradient도 얻을 수 있어 중간 layer의 학습을 도울 수 있다. 구체적으로 보면 GoogleNet 학습 시, 각 auxiliary classifier의 Loss를 모두 합친 뒤 average를 연산한다. 너무 layer가 깊으면 backpropagation중에 gradient vanishing 문제가 발생할 수 있으나 중간에 연산 되어 있는 Auxiliary Loss값을 활용하면 이러한 문제를 일부분 해결할 수 있다. 쉽게 설명하자면 computational graph 상에 서로 다른 출력이 있는 것인데 각 출력에서의 gradient를 모두 연산 한 다음 한번에 Backpropagation을 한다. 전체 Network에서 가중치를 가진 layer는 총 22개로 각 Inception Module은 1X1, 3X3, 5X5 Conv layer를 병렬적으로 가지고 있다.

#### ResNet(2015)

ResNet Architecture는 152 layer로 기존 Network에 비해 엄청나게 깊이가 깊어졌다. ResNet은 residual connections라는 방법을 사용하는데 이에 대해서 중점적으로 다룰 것이다.

그들이 처음 시작한 질문은 일반 CNN을 깊고 더 깊게 쌓게 되면 어떤 일이 발생할지 였다. 가령 VGG에 Conv, Pool layer를 깊게만 쌓는다고 과연 성능이 더 좋아지는 것일까? 대답은 No다. 20 layer와 56 layer의 Network를 한번 비교해보자.

![image-20240204185906442](/images/2024-02-04-cs231n9/image-20240204185906442.png)

두 model 다 평범한 CNN이다. 오른쪽의 test error의 경우 56 layer가 20 layer보다 좋지 않다. 하지만 train error가 조금 이상하다. 다시 한번 20/56 layer를 한번 비교해보자. 56 layer Network 의 경우 엄청나게 많은 parameter로 인해 overfitting이 발생할 것을 예상했을 것이다. 그리고 overfitting이 발생한다면 test error는 높더라도 train error는 낮아야 정상일 것이다. 그런데 56 레이어 네트워크의 traing error을 보자하니 20 layer보다 안좋다. 따라서 더 깊은 Model임에도 test 성능이 낮은 이유가 over-fitting때문이 아니라는 것을 알 수 있다.

Resnet 저자들이 내린 가설은 더 깊은 모델을 학습할 시 optimization에 문제가 생긴다는 것이 다. 모델이 깊어질수록 최적화가 어려워진다는 가설이다. 그들은 모델이 더 깊다면 적어도 더 얕 은 모델만큼은 성능이 나와야 한다고 가정했다. 가령 이런 해결책을 생각해 볼 수 있다. 우선 더 얕은 모델의 가중치를 깊은 모델의 일부 layer에 복사한 다음 나머지 layer는 identity mapping(input을 그대로 output으로 내보내는 것)을 하는 것이다. 이렇게 구성하면 shallow layer(얕은 layer)만큼의 성능이 나와야 한다. 이 아이디어를 우리가 만들 model에 녹이려면 어떻 게 model architecture를 디자인해야 할까?

![image-20240204185922935](/images/2024-02-04-cs231n9/image-20240204185922935.png)

그들의 아이디어는 layer를 단순하게 쌓지 않는 것이다. Direct mapping 대신에 Residual mapping을 하도록 블록을 쌓는 것이다. 오른쪽 그림의 입력은 그저 이전 layer에서 흘러 들어온 입력이다. 그리고 layer가 직접 H(x)를 학습하기 보다 이런 식으로 F(x)+x를 학습할 수 있도록 만 들어준다. 이를 위해 Skip Connection을 도입하게 된다. 오른쪽의 skip connection은 가중치가 존재하지 않아 입력으로 identity mapping으로 그대로 출력으로 내보내게 된다. 그러면 실제 layer는 F(x)만 학습하면 된다. 입력 x에 대한 residual(잔여)라고 볼 수 있다.
$$
H(x)=X(input) + F(x)(변화량(Residual))
$$
이 방법을 사용하면 학습이 더 쉬워지게 된다. 가령 Input = output이어야 하는 상황이라면 layer 의 출력인 F(x)(residual)가 0이어야 하므로 모든 가중치를 0으로 만들어주면 그만이다. 손쉽게 출력을 Identtiy로 만들어 줄 수 있는 것이다. 이 방법을 사용하면 앞서 제시한 방법을 손쉽게 구 성할 수 있다. Network은 residual만 학습하면 그만이다. 출력 값도 결국엔 입력 X에 가까운 값 이 된다. Layer가 Full mapping을 학습하는 것보다 이런 조금의 변화만 학습하는 것이다.

왜 Direct mapping(H(x))를 학습하는 것보다 Residual(F(x))를 학습하는 것이 더 쉬울까? 이 모든 것은 그저 가설일 뿐이다. Residual을 학습하는 것은 X에 대한 delta(변화량)를 학습시키는 것이 다. 만약 가설이 참이라면, 내 모델의 일부는 학습된 shallow layers이고 나머지 layer들은 Identity mapping이라서 잘 동작해야만 한다. 가령 Output = input (identity)이어야 하는 상황이 라면 F(x)=0이 되면 그만이다. 이는 상대적으로 학습하기 쉽다고 볼 수 있다. 이런 방식으로 Identity mapping에 가까운 값을 얻을 수 있다. 하지만 그들의 직관과 가설이 입증된 바는 없다. 그리고 일부 사람들은 ResNet에서 그 가설(residuals)이 존재할 필요가 없다고 주장하기도 한다. 그런데 실제로 ResNet을 쓰면 성능이 더 좋아진다.

![image-20240204190006417](/images/2024-02-04-cs231n9/image-20240204190006417.png)

기본적으로 ResNet은 Residual block을 쌓아 올리는 구조다. 하나의 Residual block은 2개의 3X3 Conv layer로 구성되어 있다. 이렇게 구성해야 잘 동작하기 때문이다. (이유는 모른다) 이 Residual block을 아주 깊게 쌓아 올린다. ResNet은 150 layers까지 쌓아 올릴 수 있다. 그리고 주기적으로 filter를 2배씩 늘리고 stride 2를 활용해 Downsampling을 수행한다. 초반에는 Conv layer가 추가적으로 붙으며 Network의 끝에는 FC-layer가 존재하지 않고 Global average pooling layer가 존재한다. 하나의 map 전체를 average pooling하는 것이다. 그리고 마지막에는 1000개의 class분류를 위한 노드가 붙는다. 또한 ResNet의 경우 모델 Depth가 50 이상일 때 Bottleneck Layers를 도입한다. 추가적으로 ResNet은 모든 Conv layer 다음에 Batch Normalization을 사용하며 초기화에는 Xavier를 사용하는데 추가적인 scaling factor를 추가한다 (표준 편차에 2를 곱하는 He Initialization). 이 방법은 SGD + Momentum에서 좋은 초기화 성능 을 보여준다. Learning rate는 learning rate 스케줄링을 통해 validation error가 줄어들지 않는 시 점(좋은 HP의 범위 내)에서 조금씩 줄인다. Mini-batch size는 256이고 weight decay도 적용한다. Dropout은 사용하지 않았다.

#### Model별 비교 요약

![image-20240204190030918](/images/2024-02-04-cs231n9/image-20240204190030918.png)

왼쪽 그래프는 모델의 성능 별로 정렬해 보았다. Top-1 Accuracy가 기준이고 높을수록 좋은 모델 이다. 가장 좋은 모델은 ResNet + InceptionNet model이다. 오른쪽 그래프는 계산 복잡성을 추가 한 성능을 평가한 것인데 Y축은 Top-1 accuracy이고 X축은 연산량을 나타낸다. 오른쪽으로 갈수 록 연산량이 많아진다. 원의 크기는 메모리 사용량이다.

![image-20240204190039767](/images/2024-02-04-cs231n9/image-20240204190039767.png)

다른 관점에서 분석해보자, 왼쪽 그래프는 forward pass 소요시간이다. 단위는 ms인데 VGG가 제 일 오래 걸린다. 200ms로 초당 이미지를 5개정도 처리할 수 있다. 오른쪽은 전력소모량을 나타낸 것이다.

#### Other architecture

###### Network in Network(NiN)

![image-20240204190126351](/images/2024-02-04-cs231n9/image-20240204190126351.png)

Network in Network의 기본 아이디어는 MLP Conv layer이다. Network안에 작은 Network를 만 드는 것이다. 각 Conv layer안에 MLP를 쌓아서 FC-layer 몇 개를 쌓아 abstract features를 더 잘 뽑을 수 있도록 한다. 단순히 Conv filter만 사용해 특징을 추출하지 말고, 조금 더 복잡한 계층을 만들어서 activation map을 출력하는 아이디어이다. Network in Network는 GoogLeNet과 ResNet보다 먼저 Bottleneck 개념을 정립했기 때문에 아주 의미 있는 아이디어이다.

###### ResNet과 관련된 일련의 연구들

![img](/images/2024-02-04-cs231n9/clip_image002-1707040895880-11.jpg)

1. ResNet의 저자들은 ResNet의 블록 디자인을 향상시킨 논문을 발 표했다. 이 논문에서는 ResNet block path를 조절하였는데 새로운 구 조는 direct path를 늘려서 정보들이 앞으로 더욱 더 잘 전달되고 Backpropagation도 더 잘 작동하도록 개선했다.

2. Wide Residual Network

 ![image-20240204190230517](/images/2024-02-04-cs231n9/image-20240204190230517.png)

 그들은 기존의 ResNet 논문은 깊게 쌓는 것에 열중했지만 사실 중요한 것은 depth가 아닌 residual이라고 주장했다. Residual connection이 있다면 Network가 굳이 더 깊어질 필요가 없다 고 주장한 것이다. 그래서 그들은 residual block을 더 넓게 만들♘다. 즉, Conv layer의 filter를 더 많이 추가한 것이다. 가령 기존의 ResNet에는 Block당 F개의 filter만 있♘다면 대신에 F*K개 의 filter를 구성한 것이다. 각 layer를 넓게 구성했더니 가 50 layer만 있어도 152 layer의 기존 ResNet보다 성능이 좋다는 것을 입증했다. 그리고 네트워크의 Depth 대신에 filter의 Width를 늘리면 추가적인 이점이 있는데, 계산 효율이 증가하게 된다. 왜냐하면 병렬화가 더 잘되기 때문 이다. 네트워크의 Depth를 늘리는 것은 sequential(순서대로 일어나는) 증가이기 때문에 Conv의 filter를 늘리는(width) 편이 더 효율적이다. 이들은 Network의 width/depth/residual connection 을 고찰하는 연구를 한 셈이다.

3. ResNeXt

![image-20240204190256080](/images/2024-02-04-cs231n9/image-20240204190256080.png)

여기에서도 계속 residual block의 width를 파고든다. filter의 수를 늘리는 것이다. 각 Residual block 내에 "다중 병렬 경로"를 추가한다. 이들은 pathways의 총 합을 cardinality라고 불렀다. 하나의 bottleneck ResNet block은 비교적 작지만 이런 thinner blocks을 병렬로 여러 개 묶♘다. 여기에서 ResNeXt과 Wide ResNet과의 연관성을 볼 수 있다. 또한 여러 Layers를 병렬로 묶어준 다는 점에서 Inception Module과도 연관이 있다고 볼 수 있다. ResNeXt 라는 이름 자체가 이를 내포하고 있다.

###### Stochastic Depth

주제는 Depth이다. 네트워크가 깊어지면 깊어질수록 gradient Vanishing 문제가 발생한다. 깊은 네트워크에서는 Gradient를 뒤로 전달할수록 점점 gradient가 작아지는 문제가 있다. 기본 아이 디어는 Train time에 layer의 일부를 제거하는 것이다. short network면 training이 더 잘 될 수 있기 때문이다. 일부 네트워크를 골라서 identity connection으로 버린다. 이렇게 shorter network를 만들어서 Train하면 gradient가 더 잘 전달될 수 있다. 아주 효율적인 방법이 될 수 있다. Dropout과 유사하다. 그리고 Test time에서는 full deep network를 사용한다.

###### Beyond ResNet 지향 (Non-ResNet)

![image-20240204190340335](/images/2024-02-04-cs231n9/image-20240204190340335.png)

Non-ResNet중에서도 ResNet과 견줄만한 성능의 model들이 있다. 그들 중 하나는 FractalNet 이다. 그들은 residual connection이 쓸모없다고 주장한다. 그들은 shallow/deep network의 정보 모두를 잘 전달하는 것이 중요하다고 생각했다. FractalNet에는 다양한 경로가 존재하지만 Train time에는 Dropout처럼 일부 경로만을 이용해서 Train한다. 그리고 Test time에는 full network를 사용한다. 그들은 FractalNet의 좋은 성능을 입증하는데 성공했다.

![image-20240204190350373](/images/2024-02-04-cs231n9/image-20240204190350373.png)

DenseNet (Densely Connected Convolutional Networks)도 Non-ResNet 중 하나라 볼 수 있는데 DenseNet에는 Dense Block이 있다. 한 layer가 그 layer 의 하위의 모든 layer와 연결되어 있는데 Network의 입력이미지가 모든 layer의 입력으로 들어가게 된다. 그리고 모든 layer의 출력이 각 layer의 출력과 concatenate한다. 그리고 이 값이 각 Conv layer의 입력으로 들어간다. 이 과 정에서 dimension을 줄여주는 과정이 포 함된다. 그리고 이들은 Dense Connection이 gradient Vanishing 문제 를 완화시킬 수 있다고 주장한다. 그리고 Dense connection은 Feature를 더 잘 전달하고 더 잘 사용할 수 있게 해준다고 주장한다. 각 layer의 출력이 다른 layer에서도 여러 번 사용될 수 있기 때문이다.

###### Model의 efficiency

GoogleNet은 효율적인 Mobel에 대한 방향성을 제시했다. Practical usage를 위해서는 매우 중요 한 주제이다. 효율성을 강조하는 또 하나의 Model이 있다. 바로 SqeezeNet이다. 아주 효율적인 Network이다. 그들은 fire modules라는 것을 도입했다. "squeeze layer"는 1x1 filter들로 구성되 고, 이 출력 값이 1x1/3x3 filter들로 구성되는 "expand layer"의 입력이 된다. SqueezeNet는 ImageNet에서 AlexNet 만큼의 Accuracy이지만 parameter가 50배 적다. 그리고 SqueezeNet을 더 압축하면 AlexNet보다 500배 더 작아지게 된다. SqueezeNet의 용량은 0.5MB 밖에 안된다.
